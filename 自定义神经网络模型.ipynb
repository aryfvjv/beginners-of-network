{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c38d75c",
   "metadata": {},
   "source": [
    "   已经过去一个月了，但是才刚刚开始理解师兄说的这个从状态到动作的映射，之前我一直以为从状态到到动作的映射是指具体学习的内容要求需要数据学习之后是从状态到动作的映射，就比如对于一个飞行器来说，通过神经网络学习其动作数据，比如其速度，加速度，倾斜角，攻角等来学习之后得到下一时刻的飞行器的位置，所以最初的时候，我花了较多的时间去寻找类似这样的数据，然后也尝试去试做了一下，但结果都不是很理想。后来我利用自己写的一个简单的代码得出了有关于一个简易飞行器得空间坐标位置数据，然后我尝试了利用神经网络学习原始数据，但是其实这里面有个问题，就是我生成的数据其实分别是x,y,z的一系列线性数据，由于这一系列数据前后都是成线性的，在神经网络学习的时候，我不太清楚这个标签（label）怎么设置，因为通过我的理解，神经网络学习的标签一方面可以指导神经网络进行学习，即根据输入的数据和对应的标签值调整其权重和参数，以最小化预测值于实际标签之间的差异。另一方面，在训练过程中和训练过程后，标签都用于评估神经网络的性能。所以，在写那一部分的代码中，我就一直很困恼，这个标签怎么去弄，后来应该是把z/y的数据当作标签学习（有点儿忘了，一个月前写的，代码里面有），所以最后那个训练过程学习出来，损失函数以及学习结果非常不理想。\n",
    "   最近，我在古月居上看到了一篇关于强化学习中，策略选择和决策问题讨论的文章。文章指出，在强化学习中，策略（policy)是指智能体（agent）决策的规则。策略（policy)是从状态（state)到动作（action)的映射，它定义了在给定状态下，智能体应该采取什么样的动作，策略是可以确定性的，也可以是随机性的。这个解释对我重新理解从状态到动作的映射有很大的帮助，也让我及时意识到了我之前对于师兄给出的任务完成其实是陷入了一个误区。\n",
    "   阅读全文后，我发现自定义模型实例并传入Policy,这种方式是很灵活的，可以设计任意结构的神经网络。这篇文章详细介绍了自定义模型，然后将模型应用在Policy中的详细步骤，这里我就不在赘述了。另外文件“RF.LOST”代码原始数据是csv.文件格式，虽然学习没有成功，但是后面我还会尝试几次对于csv数据预处理然后进行学习的训练，同时，也会逐步接触对于图像分类以及其它类型的神经网络的学习。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea41e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch_cpu]",
   "language": "python",
   "name": "conda-env-.conda-pytorch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
