# 深度强化学习（DRL）中的探索策略（核心组件）

DRL，通过智能体与环境的互动来学习行动的策略的机器学习方法。探索策略的主要任务是帮助智能体在环境中探索未知的状态和行为，以便在学习过程中发现更好的行为策略。在我们希望智能体能尽快找到最佳解决方案。以最大回报率为优化目标的现代强化学习算法可以非常高效地实施“利用”这个过程。探索是指探索未知的环境状态和行为，以便找到更好的策略。利用已知的环境状态和行为，以便优化策略。

## 经典的探索策略问题（Policy;state,-->action)

贪婪算法，上信心界算法，玻尔兹曼探索，汤普森采样等，都是针对智能体地一些经典探索（决策）算法。如果用神经网络模型来进行函数拟合，增加一些优化策略可以帮助模型更好的学习。比如，在函数中加入熵，鼓励智能体采用更加多样化的动作（action）；还有基于噪声的探索，在观察到的状态，动作甚至参数空间中加入噪声让动作多样化。

## 核心概念

1.状态空间（state space）：环境中所有可能的状态的集合

2.行为空间（action space）：环境中所有可能的行为的集合

3.奖励函数（Reward function):环境给出的奖励，用于评估策略的好坏。
4.策略（Policy):一个映射，将状态映射到行为。

5.值函数(Value function)：一个映射，将状态映射到期望的累计奖励。

### 联系

1.状态空间和行为空间是环境的基本组成部分，用于描述环境的动态过程。

2.奖励函数是环境给出的反馈，用于指导策略的学习与优化。

3.策略是DRL中的核心概念，用于描述代理在环境中的行为。

4.值函数是策略的一个度量标准，用于评估策略的好坏。

## 区分探索策略与利用策略

探索策略：目标在于帮助智能体在环境中探索未知的状态和行为，以便在学习过程中发现更好的行为策略。

利用策略：旨在帮助智能体根据当前已知的行为策略在环境中取得更好的表现。

两者通常结合使用，以实现完整的决策策略（policy)